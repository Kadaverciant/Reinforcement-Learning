{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od-IHzGCC2sr"
      },
      "source": [
        "# [VGG](https://arxiv.org/pdf/1409.1556.pdf)\n",
        "\n",
        "Implement VGG16, for that write specific `nn.Module`, `VGGBlock` implementing block of VGG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "ITvQtQzDC1MN"
      },
      "outputs": [],
      "source": [
        "class VGGBlock2(nn.Module):\n",
        "    def __init__(self, in_channels, out_chanels):\n",
        "        super(VGGBlock2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_chanels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_chanels, out_chanels, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation= 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        return self.pool(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VGGBlock3(nn.Module):\n",
        "    def __init__(self, in_channels, out_chanels):\n",
        "        super(VGGBlock3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_chanels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_chanels, out_chanels, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(out_chanels, out_chanels, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation= 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        return self.pool(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(VGG16, self).__init__()\n",
        "\n",
        "        self.main_branch = nn.Sequential(\n",
        "            VGGBlock2(in_channels, 64),\n",
        "            VGGBlock2(64, 128),\n",
        "            VGGBlock3(128, 256),\n",
        "            VGGBlock3(256, 512),\n",
        "            VGGBlock3(512, 512),\n",
        "        )\n",
        "\n",
        "        self.linLayer1 = nn.Linear(25088, 4096)\n",
        "        self.linLayer2 = nn.Linear(4096, 4096)\n",
        "        self.linLayer3 = nn.Linear(4096, 1000)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.main_branch(x)\n",
        "        x = self.dropout(F.relu(self.linLayer1(x)))\n",
        "        x = self.dropout(F.relu(self.linLayer2(x)))\n",
        "        x = self.linLayer3(x)\n",
        "        return F.softmax(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VGG16(\n",
              "  (main_branch): Sequential(\n",
              "    (0): VGGBlock2(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (1): VGGBlock2(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (2): VGGBlock3(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (3): VGGBlock3(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (4): VGGBlock3(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (linLayer1): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "  (linLayer2): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "  (linLayer3): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = VGG16()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gi43r1dPDRp7"
      },
      "source": [
        "# [GoogLeNet](https://arxiv.org/pdf/1409.4842.pdf)\n",
        "\n",
        "## Inception module\n",
        "\n",
        "Write specific `nn.Module` for Inception module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_chanels, **kwargs):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_chanels, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_chanels)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return F.relu(self.bn(self.conv(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InceptionModule(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_channels=256, \n",
        "        out_1x1=128,\n",
        "        red_3x3=64,\n",
        "        out_3x3=192,\n",
        "        red_5x5=64,\n",
        "        out_5x5=96,\n",
        "        out_pool=64,\n",
        "    ):\n",
        "        super(InceptionModule, self).__init__()\n",
        "        self.branch1 = ConvBlock(in_channels, out_1x1, kernel_size=1)\n",
        "        self.branch2 = nn.Sequential(\n",
        "            ConvBlock(in_channels, red_3x3, kernel_size=1, padding=0),\n",
        "            ConvBlock(red_3x3, out_3x3, kernel_size=3, padding=1),\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            ConvBlock(in_channels, red_5x5, kernel_size=1),\n",
        "            ConvBlock(red_5x5, out_5x5, kernel_size=5, padding=2),\n",
        "        )\n",
        "        self.branch4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n",
        "            ConvBlock(in_channels, out_pool, kernel_size=1),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        branches = (self.branch1, self.branch2, self.branch3, self.branch4)\n",
        "        return torch.cat([branch(x) for branch in branches], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "InceptionModule(\n",
              "  (branch1): ConvBlock(\n",
              "    (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (branch2): Sequential(\n",
              "    (0): ConvBlock(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): ConvBlock(\n",
              "      (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (branch3): Sequential(\n",
              "    (0): ConvBlock(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): ConvBlock(\n",
              "      (conv): Conv2d(64, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "      (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (branch4): Sequential(\n",
              "    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
              "    (1): ConvBlock(\n",
              "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = InceptionModule()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pot5itHXDisN"
      },
      "source": [
        "## Stem network\n",
        "\n",
        "Write down, why do we need a Stem network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhADh2IVDojD"
      },
      "source": [
        "In order to quickly and strongly reduce the spatial dimensions (compress the image before parallel processing) in order to minimize the number of elements in the layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD7dsr2yDuGC"
      },
      "source": [
        "# [ResNet](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "\n",
        "Implement ResNet-18, for that write specific `nn.Module`, `ResNetBlock` implementing block of ResNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
        "        super(ResNetBlock, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
        "                        nn.BatchNorm2d(out_channels),\n",
        "                        nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(\n",
        "                        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                        nn.BatchNorm2d(out_channels))\n",
        "        self.downsample = downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        return F.relu(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResNet18(nn.Module):\n",
        "    \n",
        "    def init(self, in_channels=3, out_channels=1000):\n",
        "        super(ResNet18, self).init()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "        self.blocks = nn.Sequential(\n",
        "            ResNetBlock(64, 64, stride=1), \n",
        "            ResNetBlock(64, 64),\n",
        "            ResNetBlock(64, 128, downsample=self.downsampleF(64, 128), stride=2), \n",
        "            ResNetBlock(128, 128),\n",
        "            ResNetBlock(128, 256, downsample=self.downsampleF(128, 256), stride=2), \n",
        "            ResNetBlock(256, 256),\n",
        "            ResNetBlock(256, 512, downsample=self.downsampleF(256, 512), stride=2), \n",
        "            ResNetBlock(512, 512),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, out_channels)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.blocks(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x \n",
        "    \n",
        "    def downsampleF(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1), \n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNet18()\n"
          ]
        }
      ],
      "source": [
        "model = ResNet18()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAkVh_wLES6U"
      },
      "source": [
        "# [ResNeXt](https://arxiv.org/pdf/1611.05431.pdf)\n",
        "\n",
        "Write specific `nn.Module`, `ResNeXtBlock` implementing block of ResNeXt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "6FN7E2GADnTI"
      },
      "outputs": [],
      "source": [
        "class ResNeXtBlock(nn.Module):\n",
        "    def __init__(self, in_channels, stride = 1, downsample = None):\n",
        "        super(ResNeXtBlock, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels//2, kernel_size=1, stride=1),\n",
        "            nn.BatchNorm2d(in_channels//2),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels//2, in_channels//2, kernel_size=3, stride=1, padding=1, groups=32),\n",
        "            nn.BatchNorm2d(in_channels//2),\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels//2, in_channels, kernel_size=1, stride=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return self.conv3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ResNeXtBlock(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = ResNeXtBlock(512)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebgJY9YcEw2U"
      },
      "source": [
        "# [SENet](https://arxiv.org/pdf/1709.01507.pdf)\n",
        "\n",
        "Write specific `nn.Module`, `SEBlock` implementing block of SENet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "BfddhrMkE5un"
      },
      "outputs": [],
      "source": [
        "class SEBlock(nn.Module):\n",
        "    \"credits: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py#L4\"\n",
        "    def __init__(self, c, r=16):\n",
        "        super().__init__()\n",
        "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "        self.excitation = nn.Sequential(\n",
        "            nn.Linear(c, c // r, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(c // r, c, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, c, _, _ = x.shape\n",
        "        y = self.squeeze(x).view(bs, c)\n",
        "        y = self.excitation(y).view(bs, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SEBlock(\n",
            "  (squeeze): AdaptiveAvgPool2d(output_size=1)\n",
            "  (excitation): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=16, bias=False)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=16, out_features=256, bias=False)\n",
            "    (3): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = SEBlock(256)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px-Os_viE-aM"
      },
      "source": [
        "# [Neural Architecture Search](https://arxiv.org/pdf/1611.01578.pdf)\n",
        "\n",
        "For the neural network of your assignment 2, write down the parametrization of the network you would use for the NAS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sojINj1YFeMU"
      },
      "source": [
        "I would parametrize the size of the hidden layers. It doesn't seems to be wery usefull in my implementation, since I don't use convolutions in it (work just with set of numbers)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "5a72086affac0942e77ca8d313fe7cee3cbeae03ce179ef0541d0335cc6ec24c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
